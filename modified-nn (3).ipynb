{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modified Neural Network on MNIST Dataset","metadata":{}},{"cell_type":"code","source":"#Load the dataset\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:39.393228Z","iopub.execute_input":"2024-03-20T07:57:39.393630Z","iopub.status.idle":"2024-03-20T07:57:39.450868Z","shell.execute_reply.started":"2024-03-20T07:57:39.393600Z","shell.execute_reply":"2024-03-20T07:57:39.449996Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-dataset/train-images.idx3-ubyte\n/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte\n/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte\n/kaggle/input/mnist-dataset/train-labels.idx1-ubyte\n/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\n/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\n/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte\n/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte\n","output_type":"stream"}]},{"cell_type":"code","source":"#Import the dataset and other python libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport struct\nfrom array import array\nfrom os.path import join","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:39.452603Z","iopub.execute_input":"2024-03-20T07:57:39.453101Z","iopub.status.idle":"2024-03-20T07:57:39.929203Z","shell.execute_reply.started":"2024-03-20T07:57:39.453072Z","shell.execute_reply":"2024-03-20T07:57:39.928042Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class MnistDataloader(object):\n    def __init__(self, training_images_filepath, training_labels_filepath,\n                 test_images_filepath, test_labels_filepath):\n        self.training_images_filepath = training_images_filepath\n        self.training_labels_filepath = training_labels_filepath\n        self.test_images_filepath = test_images_filepath\n        self.test_labels_filepath = test_labels_filepath\n\n    def read_images_labels(self, images_filepath, labels_filepath):\n        labels = []\n        with open(labels_filepath, 'rb') as file:\n            magic, size = struct.unpack(\">II\", file.read(8))\n            if magic != 2049:\n                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n            labels = array(\"B\", file.read())\n\n        with open(images_filepath, 'rb') as file:\n            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n            if magic != 2051:\n                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n            image_data = array(\"B\", file.read())\n        images = []\n        for i in range(size):\n            images.append([0] * rows * cols)\n        for i in range(size):\n            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n            img = img.reshape(28, 28)\n            images[i][:] = img\n\n        return images, labels\n\n    def load_data(self):\n        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n        return (x_train, y_train), (x_test, y_test)\n    \n# Set file paths based on added MNIST Datasets\ninput_path = '../input'\ntraining_images_filepath = join(input_path, 'mnist-dataset/train-images.idx3-ubyte')\ntraining_labels_filepath = join(input_path, 'mnist-dataset/train-labels.idx1-ubyte')\ntest_images_filepath = join(input_path, 'mnist-dataset/t10k-images.idx3-ubyte')\ntest_labels_filepath = join(input_path, 'mnist-dataset/t10k-labels.idx1-ubyte')\n\n# Load MINST dataset\nmnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:39.930745Z","iopub.execute_input":"2024-03-20T07:57:39.931338Z","iopub.status.idle":"2024-03-20T07:57:42.937211Z","shell.execute_reply.started":"2024-03-20T07:57:39.931300Z","shell.execute_reply":"2024-03-20T07:57:42.935972Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Display a few images from the training set\nnum_images_to_display = 5\nfor i in range(num_images_to_display):\n    plt.subplot(1, num_images_to_display, i + 1)\n    plt.imshow(x_train[i], cmap='gray')\n    plt.title(f\"Label: {y_train[i]}\")\n    plt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:42.939695Z","iopub.execute_input":"2024-03-20T07:57:42.940042Z","iopub.status.idle":"2024-03-20T07:57:43.506339Z","shell.execute_reply.started":"2024-03-20T07:57:42.940013Z","shell.execute_reply":"2024-03-20T07:57:43.505451Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 5 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Display a few images from the testing set\nnum_images_to_display = 5\nfor i in range(num_images_to_display):\n    plt.subplot(1, num_images_to_display, i + 1)\n    plt.imshow(x_test[i], cmap='gray')\n    plt.title(f\"Label: {y_test[i]}\")\n    plt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.507478Z","iopub.execute_input":"2024-03-20T07:57:43.508957Z","iopub.status.idle":"2024-03-20T07:57:43.869756Z","shell.execute_reply.started":"2024-03-20T07:57:43.508914Z","shell.execute_reply":"2024-03-20T07:57:43.868275Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 5 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Base class\nclass Layer:\n    def __init__(self):\n        self.input = None\n        self.output = None\n        self.bias = None\n        self.zeta = None\n        self.z = None \n\n    # computes the output Y of a layer for a given input X\n    def forward_propagation(self, input):\n        raise NotImplementedError\n\n    # computes dE/dX for a given dE/dY (and update parameters if any)\n    def backward_propagation(self, output_error, learning_rate):\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.871593Z","iopub.execute_input":"2024-03-20T07:57:43.872042Z","iopub.status.idle":"2024-03-20T07:57:43.880953Z","shell.execute_reply.started":"2024-03-20T07:57:43.872000Z","shell.execute_reply":"2024-03-20T07:57:43.879423Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Forward propagation for each layer\n\n        #Z(i) = 0.2 * [{j=1to J} δ ((200 b (j, i) - 100) * (y(j) -  ζ (j, i))) - J + 5] \n        \n        #y(i) = ReLU (Zi)\n        \nBackward Propagation for each layer:\n\n    #β(i,k) = 0.2 *  δ' [(200 b(i, k) - 100) (yi - ζ (i , k))] * (200 b(i,k) - 100)\n\n    #θ (i, k) = 0.2 *  δ' [(200 b(i, k) - 100) (yi - ζ (i , k))] * 200 * (yi - ζ (i, k))\n\n    #∇ b(i, k) =  { (yk – tk) * θ (i, k)}, if z(k) > 0 \n\n    #∇ b(i, k) =  { 0 }, if z(k) < 0 ","metadata":{}},{"cell_type":"markdown","source":"Questions:\n\n1)\tDuring the Backproagation, the Thresold gradient of the last layer is used. Do we need to accumulate it for all the samples and layer like:\nself.prev_zeta_gradient += zeta_gradient\n\nOr do we just need it for the previous layer?\nself.prev_zeta_gradient = zeta_gradient\n\n\n2)\tDuring the backpropagation, we need ReLu prime/ ReLu derivative like in normal NN? \nCurrently it is being used. If the input to ReLU is positive, the derivative is 1; if the input is negative, the derivative is 0. This derivative is multiplied by the gradient of the loss with respect to the output of the ReLU layer.\n\n3)\t/tmp/ipykernel_33/912799544.py:11: RuntimeWarning: overflow encountered in exp\n  \tsig = 1 / (1 + np.exp(-alpha * x))\n\nObservations:\n\n•Transposed the self.z to adjust with the shapes of bias and thau.\noEither need to change the shape of the output or the thau to make them compatible during calculation.\noIf I do not transpose the self.z during the output, then for the next layers, incompatible operands error occur during the forward propagation while doing: self.input - self.zeta \noValueError: operands could not be broadcast together with shapes (50,1) (10,50) \nshape of self.z:  (50, 1)\nshape of input_size:  (50, 1)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\n\n•If the shape of the output is changed by making it self.z.T, then during the error calculation, need to change the shape of the y_train as well.","metadata":{}},{"cell_type":"code","source":"# inherit from base class Layer: Fully connected layer code\nclass FCLayer(Layer):\n    # input_size = number of input neurons\n    # output_size = number of output neurons\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n#         print('input_size: ', input_size)\n#         print('output_size: ', output_size)\n        \n         # Initialize zeta based on the layer's position\n#         if input_size == 784:  # First layer\n#             self.zeta = np.random.rand(1, input_size) - 0.5\n#             self.bias = np.random.rand(1, input_size) - 0.5\n#         else:  # Subsequent layers\n#             self.zeta = np.random.rand(input_size, output_size) - 0.5\n#             self.bias = np.random.rand(input_size, output_size) - 0.5\n\n        self.zeta = np.random.rand(output_size, input_size) - 0.5\n        self.bias = np.random.rand(output_size, input_size) - 0.5\n#         self.prev_zeta_gradient = np.zeros_like(input_size)\n       \n\n    # returns output for a given input\n    def forward_propagation(self, input_data):\n        self.input = input_data\n#         print('type of input_size: ', type(input_data))\n        print('shape of input_size: ', input_data.shape)\n#         print('type of self.zeta: ', type(self.zeta))\n        print('shape of self.zeta: ',self.zeta.shape)\n#         print('type of self.bias: ',type(self.bias))\n        print('shape of self.bias: ',self.bias.shape)\n        self.z = 0.2 * (np.sum(sigmoid((200 * self.bias - 100) * (self.input - self.zeta)) - self.output_size + 5, axis = 1, keepdims=True))\n        self.output =  self.z.T\n        print('shape of self.z: ',self.output.shape)\n#         print('self.z: ',self.z)\n        return self.output\n\n    \n    # computes thresholds - zeta, zeta_gradient and bias_gradient\n    def backward_propagation(self, is_last_layer, layer_target_output, prev_zeta_gradient):\n        #calculate the beta and theta for the previous layer\n        beta = 0.2 * sigmoid_prime((200 * self.bias - 100) * (self.input - self.zeta)) * (200 * self.bias - 100)\n        theta = 0.2 * sigmoid_prime((200 * self.bias - 100) * (self.input - self.zeta)) * 200 * (self.input - self.zeta)\n\n#         print('Shape of prev_zeta_gradient before any calculation:', self.prev_zeta_gradient.shape)\n        \n        \n#         print('type of layer_target_output: ', type(layer_target_output))\n#         print('shape of layer_target_output: ',layer_target_output.shape)\n        print('Shape of beta from BP:', beta.shape)\n        print('Shape of theta from BP:', theta.shape)\n#         print('type of self.output from BP: ', type(self.output))\n        print('shape of self.output from BP: ',self.output.shape)\n#         print('shape of layer_target_output from BP: ',layer_target_output.shape)\n\n        if is_last_layer:\n            zeta_gradient = np.where(self.z > 0,np.dot((layer_target_output - self.output) , beta), 0)\n            bias_gradient = np.where(self.z > 0,np.dot((self.output - layer_target_output) , theta), 0)\n            print('shape of zeta_gradient from the is_last_layer statement: ',zeta_gradient.shape)\n            print('shape of bias_gradient from the is_last_layer statement: ',bias_gradient.shape)\n        else:\n            np_sum = np.sum(prev_zeta_gradient, axis=0)\n            zeta_gradient = np.where(self.z > 0, np.dot(np.sum(prev_zeta_gradient, axis=0), beta), 0)\n            bias_gradient = np.where(self.z > 0, np.dot(np.sum(prev_zeta_gradient, axis=0), -theta), 0)\n            print('shape of zeta_gradient from the else statement: ',zeta_gradient.shape)\n            print('shape of bias_gradient from the else statement: ',bias_gradient.shape)\n    \n        prev_zeta_gradient = zeta_gradient\n#         print('Shape of prev_zeta_gradient after calculation:', prev_zeta_gradient.shape)\n       \n        # update parameters\n        self.bias = self.bias + bias_gradient\n        self.zeta = self.zeta + zeta_gradient\n#         print('self.bias from BP: ',self.bias)\n#         print('self.zeta from BP:  ',self.zeta)\n        return prev_zeta_gradient","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.883628Z","iopub.execute_input":"2024-03-20T07:57:43.884075Z","iopub.status.idle":"2024-03-20T07:57:43.906311Z","shell.execute_reply.started":"2024-03-20T07:57:43.884041Z","shell.execute_reply":"2024-03-20T07:57:43.904703Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# inherit from base class Layer: Activation Layer code\nclass ActivationLayer(Layer):\n    def __init__(self, activation, activation_prime):\n        super().__init__()\n        self.activation = activation\n        self.activation_prime = activation_prime\n\n    # returns the activated input\n    def forward_propagation(self, input_data):\n        self.input = input_data\n        #y(i) = ReLU (Zi)\n        self.output = self.activation(self.input)\n        return self.output\n\n    # Not returning anything for the modified NN\n    def backward_propagation(self, is_last_layer, layer_target_output, prev_zeta_gradient):\n        _ = self.activation_prime(self.input)\n        return prev_zeta_gradient","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.908105Z","iopub.execute_input":"2024-03-20T07:57:43.908562Z","iopub.status.idle":"2024-03-20T07:57:43.922364Z","shell.execute_reply.started":"2024-03-20T07:57:43.908529Z","shell.execute_reply":"2024-03-20T07:57:43.920903Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# updated activation function and its derivative\ndef ReLU(x):\n    return np.maximum(0, x)\n\ndef ReLU_prime(x):\n    return np.where(x > 0, 1, 0)\n\n# Modified Sigmoid activation function and its derivative\ndef sigmoid(x, alpha=5):\n    x = x.astype(float)\n    sig = 1 / (1 + np.exp(-alpha * x))\n    #print ('shape of sigmoid function: ', sig.shape)\n    return sig\n\ndef sigmoid_prime(x, alpha=5):\n    return alpha * sigmoid(x, alpha) * (1 - sigmoid(x, alpha))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.924094Z","iopub.execute_input":"2024-03-20T07:57:43.924581Z","iopub.status.idle":"2024-03-20T07:57:43.939038Z","shell.execute_reply.started":"2024-03-20T07:57:43.924542Z","shell.execute_reply":"2024-03-20T07:57:43.937413Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# loss function and its derivative\n# def mse(y_true, y_pred):\n#     print ('shape of y_true: ', y_true.shape)\n#     print ('shape of y_pred: ', y_pred.shape)\n#     return np.mean(np.power(y_true-y_pred, 2));\n\ndef mse(y_true, y_pred):\n    return np.mean(np.power(np.squeeze(y_true) - np.squeeze(y_pred), 2))\n    \ndef mse_prime(y_true, y_pred):\n    return 2*(y_pred-y_true)/y_true.size;","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.942730Z","iopub.execute_input":"2024-03-20T07:57:43.943231Z","iopub.status.idle":"2024-03-20T07:57:43.952093Z","shell.execute_reply.started":"2024-03-20T07:57:43.943159Z","shell.execute_reply":"2024-03-20T07:57:43.950640Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class Network:\n    def __init__(self):\n        self.layers = []\n        self.loss = None\n        self.loss_prime = None\n\n    # add layer to network\n    def add(self, layer):\n        self.layers.append(layer)\n\n    # set loss to use\n    def use(self, loss, loss_prime):\n        self.loss = loss\n        self.loss_prime = loss_prime\n\n    # predict output for given input\n    def predict(self, input_data):\n        # sample dimension first\n        samples = len(input_data)\n        result = []\n\n        # run network over all samples\n        for i in range(samples):\n            # forward propagation\n            output = input_data[i]\n            for layer in self.layers:\n                output = layer.forward_propagation(output)\n                print ('output from predict :' ,output)\n            result.append(output)\n\n        return result\n\n    # train the network\n    def fit(self, x_train, y_train, epochs, learning_rate):\n        # sample dimension first\n        prev_zeta_gradient = None\n        samples = len(x_train)\n#         print('type of y_train: ', type(y_train))\n#         print('shape of y_train: ',y_train.shape)\n\n        # training loop\n        for i in range(epochs):\n#             print('Epoch ', i)\n            err = 0\n            for j in range(samples):\n                print('Sample ', j)\n#                 print('type of y_train[j]: ', type(y_train[j]))\n#                 print('shape of y_train[3]: ',y_train[3].shape)\n                # forward propagation\n                output = x_train[j]\n                for layer in self.layers:\n#                     print('forward_propagation for ', layer)\n                    output = layer.forward_propagation(output)\n\n                #compute loss (for display purpose only)\n#                 print ('y_train: ', y_train.shape)\n#                 print ('y_train[j]: ', y_train[j].shape)\n#                 print ('y_train[1]: ', y_train[1].shape)\n                err += self.loss(y_train[j], output)\n                \n                #compute error for the back propagation\n                error = self.loss_prime(y_train[j], output)\n\n                #backward propagation\n                for layer in reversed(self.layers):\n#                     print('backward_propagation for ', layer)\n#                     print(layer)\n                    is_last_layer = (layer == self.layers[-2])  # Check if it's the last layerindex\n#                     print(\"is_last_layer:\", is_last_layer)\n                    prev_zeta_gradient = layer.backward_propagation(is_last_layer, y_train[j], prev_zeta_gradient)\n                    \n            # calculate average error on all samples\n            err /= samples\n            print('epoch %d/%d   error=%f' % (i+1, epochs, err))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.954155Z","iopub.execute_input":"2024-03-20T07:57:43.954883Z","iopub.status.idle":"2024-03-20T07:57:43.971487Z","shell.execute_reply.started":"2024-03-20T07:57:43.954847Z","shell.execute_reply":"2024-03-20T07:57:43.970161Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from keras.utils import to_categorical\n\n# training data : 60000 samples\n# Convert x_train and x_test to NumPy array\nx_train = np.array(x_train)\ny_train = np.array(y_train)\nx_test = np.array(x_test)\ny_test = np.array(y_test)\n\n# reshape and normalize input data\nx_train = x_train.reshape(x_train.shape[0], 1, 28*28)\nx_train = x_train.astype('float32')\nx_train /= 255\n\n# encode output which is a number in range [0,9] into a vector of size 10\n# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\ny_train = to_categorical(y_train)\n\n# same for test data : 10000 samples\nx_test = x_test.reshape(x_test.shape[0], 1, 28*28)\nx_test = x_test.astype('float32')\nx_test /= 255\n\n# encode test output which is a number in range [0,9] into a vector of size 10\n#y_test = to_categorical(y_test)\n\n#temp section\n\n# Select the first 1000 samples for training\nx_train_subset = x_train[:10]\ny_train_subset = y_train[:10]\n\n# Select the first 1000 samples for testing\nx_test_subset = x_test[:10]\ny_test_subset = y_test[:10]\n\n\n\n\n# test\n#out = net.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:57:43.973230Z","iopub.execute_input":"2024-03-20T07:57:43.973915Z","iopub.status.idle":"2024-03-20T07:58:01.225211Z","shell.execute_reply.started":"2024-03-20T07:57:43.973867Z","shell.execute_reply":"2024-03-20T07:58:01.223872Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-03-20 07:57:46.554494: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-20 07:57:46.554636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-20 07:57:46.732625: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Network with 4 FC layers\nnet = Network()\n\nnet.add(FCLayer(28*28, 50))                # input_shape=(1, 28*28)    ;   output_shape=(1, 50)\nnet.add(ActivationLayer(ReLU, ReLU_prime))\nnet.add(FCLayer(50, 50))                   # input_shape=(1, 50)      ;   output_shape=(1, 50)\nnet.add(ActivationLayer(ReLU, ReLU_prime))\n#net.add(FCLayer(50, 50))                   # input_shape=(1, 50)      ;   output_shape=(1, 50)\n#net.add(ActivationLayer(ReLU, ReLU_prime))\nnet.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\nnet.add(ActivationLayer(sigmoid, sigmoid_prime))\n\n# train\nnet.use(mse, mse_prime)\nnet.fit(x_train_subset, y_train_subset, epochs=2, learning_rate=0.01)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:58:01.227152Z","iopub.execute_input":"2024-03-20T07:58:01.228594Z","iopub.status.idle":"2024-03-20T07:58:01.513188Z","shell.execute_reply.started":"2024-03-20T07:58:01.228545Z","shell.execute_reply":"2024-03-20T07:58:01.511543Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Sample  0\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  1\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  2\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  3\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  4\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  5\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  6\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  7\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  8\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  9\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nepoch 1/2   error=0.100000\nSample  0\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  1\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  2\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  3\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  4\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  5\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  6\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/912799544.py:11: RuntimeWarning: overflow encountered in exp\n  sig = 1 / (1 + np.exp(-alpha * x))\n","output_type":"stream"},{"name":"stdout","text":"Shape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  7\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  8\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nSample  9\nshape of input_size:  (1, 784)\nshape of self.zeta:  (50, 784)\nshape of self.bias:  (50, 784)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (50, 50)\nshape of self.bias:  (50, 50)\nshape of self.z:  (1, 50)\nshape of input_size:  (1, 50)\nshape of self.zeta:  (10, 50)\nshape of self.bias:  (10, 50)\nshape of self.z:  (1, 10)\nShape of beta from BP: (10, 50)\nShape of theta from BP: (10, 50)\nshape of self.output from BP:  (1, 10)\nshape of zeta_gradient from the is_last_layer statement:  (10, 50)\nshape of bias_gradient from the is_last_layer statement:  (10, 50)\nShape of beta from BP: (50, 50)\nShape of theta from BP: (50, 50)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 50)\nshape of bias_gradient from the else statement:  (50, 50)\nShape of beta from BP: (50, 784)\nShape of theta from BP: (50, 784)\nshape of self.output from BP:  (1, 50)\nshape of zeta_gradient from the else statement:  (50, 784)\nshape of bias_gradient from the else statement:  (50, 784)\nepoch 2/2   error=0.100000\n","output_type":"stream"}]}]}