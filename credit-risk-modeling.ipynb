{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1285766,"sourceType":"datasetVersion","datasetId":742347}],"dockerImageVersionId":30042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Credit risk\n### Will the Customer pay the financing or not?\n\n#### DEFINITION\n...\"Credit risk is the possibility of a loss resulting from a borrower's failure to repay a loan or meet contractual obligations. Traditionally, it refers to the risk that a lender may not receive the owed principal and interest, which results in an interruption of cash flows and increased costs for collection. Excess cash flows may be written to provide additional cover for credit risk. When a lender faces heightened credit risk, it can be mitigated via a higher coupon rate, which provides for greater cash flows.\n\nAlthough it's impossible to know exactly who will default on obligations, properly assessing and managing credit risk can lessen the severity of a loss. Interest payments from the borrower or issuer of a debt obligation are a lender's or investor's reward for assuming credit risk...\"\n\n\n#### UNDESTANDING CREDIT RISK\nWhen lenders offer mortgages, credit cards, or other types of loans, there is a risk that the borrower may not repay the loan. Similarly, if a company offers credit to a customer, there is a risk that the customer may not pay their invoices. Credit risk also describes the risk that a bond issuer may fail to make payment when requested or that an insurance company will be unable to pay a claim.\n\nCredit risks are calculated based on the borrower's overall ability to repay a loan according to its original terms. To assess credit risk on a consumer loan, lenders look at the five Cs: credit history, capacity to repay, capital, the loan's conditions, and associated collateral.1\n\nSome companies have established departments solely responsible for assessing the credit risks of their current and potential customers. Technology has afforded businesses the ability to quickly analyze data used to assess a customer's risk profile.\n\nIf an investor considers buying a bond, they will often review the credit rating of the bond. If it has a low rating (< BBB), the issuer has a relatively high risk of default. Conversely, if it has a stronger rating (BBB, A, AA, or AAA), the risk of default is progressively diminished.\n\nBond credit-rating agencies, such as Moody's Investors Services and Fitch Ratings, evaluate the credit risks of thousands of corporate bond issuers and municipalities on an ongoing basis.2ï»¿ 3ï»¿ For example, a risk-averse investor may opt to buy an AAA-rated municipal bond. In contrast, a risk-seeking investor may buy a bond with a lower rating in exchange for potentially higher returns.\n\n\n\n#### CREDIT RISK VS. INTEREST RATES\nâš¡ **Important**: if there is a higher level of perceived credit risk, investors and lenders usually demand a higher rate of interest for their capital.\n\nCreditors may also choose to forgo the investment or loan. For example, because a mortgage applicant with a superior credit rating and steady income is likely to be perceived as a low credit risk, they will receive a low-interest rate on their mortgage. In contrast, if an applicant has a poor credit history, they may have to work with a subprime lenderâ€”a mortgage lender that offers loans with relatively high-interest rates to high-risk borrowersâ€”to obtain financing. The best way for a high-risk borrower to acquire lower interest rates is to improve their credit score; those struggling to do so might want to consider working with one of the best credit repair companies.\n\nSimilarly, bond issuers with less-than-perfect ratings offer higher interest rates than bond issuers with perfect credit ratings. The issuers with lower credit ratings use high returns to entice investors to assume the risk associated with their offerings.\n\n\n\n*Source*: https://www.investopedia.com/terms/c/creditrisk.asp\n","metadata":{}},{"cell_type":"markdown","source":"## Data Description\n\n### Objective\n\nThe purpose of this database is to provide information about a bank's customers so that machine learning models can be developed that can predict whether a particular customer will repay the loan or not.\n\nVariable Name Description Type\n\n* clientid = Client id Person\n* income   = Income of the client\n* age      = Age of client\n* loan     = Load value.\n* default  = target 0 good client, 1 not.","metadata":{}},{"cell_type":"markdown","source":"-----------------------","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“š Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\npd.set_option('max_columns', 50)","metadata":{"ExecuteTime":{"end_time":"2019-06-05T03:55:04.610964Z","start_time":"2019-06-05T03:55:03.338924Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“¥ Load dataset","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","metadata":{"ExecuteTime":{"end_time":"2019-06-05T03:55:04.610964Z","start_time":"2019-06-05T03:55:03.338924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = pd.read_csv('/kaggle/input/credit-risk/original.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick look at the dataset\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ## Variable types\n\nI prefere to use this approach of separating variable types as a list to make clear which one is numerical and which one is categorical. In my opinion, this quick split helps to see suitable data transformation and data plots for each case. Thus, it's up to the data scientist's convenience to use it or not.","metadata":{}},{"cell_type":"code","source":"target = 'default'  \nnum_vars = list(df.columns.drop(['clientid', 'default']))\n#cat_vars = list(df.columns.drop([''])) # there's no categorical features within the dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ”Ž EDA (Exploratory Data Analysis) ","metadata":{}},{"cell_type":"markdown","source":"### Local EDA\n","metadata":{}},{"cell_type":"code","source":"# choosing any id\ndf.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  <font color=blue>ðŸ’¬ Comments\n<font color=darkblue>\n<pre></pre> \n\nI've randomly chosen two IDs to understand what happened with them through data. The client ID number **1996** is **48** years old and posses a sallary around **59221.04** dollars and a loan that represents **3.25%** of the income. On the other hand, the client ID **1998** is younger (**28** years of age) and has an income around **44311.45** and a loan that represents **12.46%**.\n\n**Data Story**: It's a database that shows us different client profiles and their financial health (how much they make and what is the debt percent they have)","metadata":{}},{"cell_type":"markdown","source":"### Overall EDA","metadata":{}},{"cell_type":"markdown","source":"### ðŸ§® Descriptive Measures","metadata":{}},{"cell_type":"code","source":"descriptive_measures = round(df.drop(['clientid','default'],axis=1).describe(),2)\ndescriptive_measures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ðŸ“Š Data Visualization","metadata":{}},{"cell_type":"markdown","source":"#### dependent variable (*target*)","metadata":{}},{"cell_type":"code","source":"# VisualizaÃ§Ã£o da variÃ¡vel binÃ¡ria (target)\ndf.default.value_counts().plot(kind = 'bar', color = ['C0', 'C1'], figsize=(6, 3), rot=0, title='PLOT: DEFAULT')\nplt.xlabel(\"EVENT: |  0=Default  |  1=No Default\")\nplt.ylabel(\"ABSOLUTE FREQUENCY (count)\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Overview of  binary variable (target)\ninad = df.default.value_counts()[0]\nad = df.default.value_counts()[1]\nprint('Total NO DEFAULT(0):', ad, 'lines, representing',round((ad/(inad+ad))*100,2),'% of the dataset')\nprint('Total DEFAULT(1):', inad, 'lines, representing',round((inad/(inad+ad))*100,2),'% of the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  <font color=blue>ðŸ’¬ Comments\n<font color=darkblue>\n<pre></pre> \n\n* The 'PLOT:DEFAULT' chart above shows that the DEFAULT event is considered a **RARE EVENT**, representing **14.15%** of the total \n* ðŸš© **ATTENTION POINT**: **UNBALANCED** data (think about some type of treatment for a model such as: Oversampling or Undersampling)","metadata":{}},{"cell_type":"markdown","source":"#### independent variable (*features*)","metadata":{}},{"cell_type":"code","source":"# code for plotting multiple HISTOGRAM\nfig, axs = plt.subplots(3,1, figsize=(10, 5),constrained_layout=True)\naxs[0].hist(df['income'],bins=100)\naxs[1].hist(df['age'], bins=100)\naxs[2].hist(df['loan'], bins=100)\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code for plotting multiple SCATTER PLOTS\nfig, axs = plt.subplots(3,1, figsize=(10,10),constrained_layout=True)\n\naxs[0].scatter(x=df['income'], y=df['default'], marker='o', color='r')\naxs[1].scatter(x=df['age'], y=df['default'], marker='o', color='r')\naxs[2].scatter(x=df['loan'], y=df['default'], marker='o', color='r')\n\n\nplt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code for plotting multiple BOXPLOTS\nfig, axes = plt.subplots(1, 3, figsize=(20, 5),constrained_layout=True)\nfig.suptitle('BOXPLOTS DE I, II and III', ha = 'right')\nmy_pal=\"Blues\"\n\nsns.boxplot(ax=axes[0], data=df, x = target, y=\"income\", palette=my_pal)\nsns.boxplot(ax=axes[1], data=df, x = target, y=\"age\", palette=my_pal)\nsns.boxplot(ax=axes[2], data=df, x = target, y=\"loan\", palette=my_pal);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  <font color=blue>ðŸ’¬ Comments\n<font color=darkblue>\n<pre></pre> \n    \n**INCOME**\n    \n- The feature varies between 20014 and 69995.7 thousand, with an income average ~ 45331.6. Important aspects when looking at data:\n     - 1) **quartiles** (Q1, Q2 and Q3) inform us of **quantity** and the respective value of **variation** around **32796.46 to 57791.28**;\n        - Interpretation Q1 tells us that 25% of data has a income up to 32796.46 and 75% of data has an income above from it\n        - Interpretation Q2 tells us that 50% of data has a income up to 45789.12 and 75% of data has an income above from it\n        - Interpretation Q3 tells us that 75% of data has a income up to 57791.28 and 25% of data has an income above from it\n     - 2) The MINIMUM value of 20014 and MAXIMUM value of 69995.7 does not indicate **POSSIBLE INCONSISTENCY IN DATA** once there is nothing within data that really catches my eye\n     - 3) No missing values (0% of missing values)\n     - 4) **Tendency**: according to scatter plot (above),there's no standard to define such tendency. There are ids within good or bad payer at any income available\n     - 5) According to graphical visualization (scatter and boxplot), the feature 'INCOME' does not seem to be a candidate variable for predictor to explain the variability of y (Feature Selection), since there's no differences between the variabilities of each label, 1 and 0. Therefore, such visualizations have the sensation of lack of \"movement\" in both charts. The ideal situation is to TEST and get the outcomes. Perhaps if we combined with other variables, this feature becomes significant for the model \n<pre> </pre>\n    \n**AGE**\n    \n- The feature varies between -52 and 64, with an age average ~ 41 years old. Important aspects when looking at data:\n     - 1) **quartiles** (Q1, Q2 and Q3) tell us of **quantity** and the respective value of **variation** around **29 to 53**;\n        - Interpretation Q1 tells us that 25% of data has an age up to 29 y.o and 75% of data has an age above from it\n        - Interpretation Q2 tells us that 50% of data has an age up to 41 and 75% of data has an income above from it\n        - Interpretation Q3 tells us that 75% of data has an age up to 53 and 25% of data has an income above from it\n     - 2) The MINIMUM age of -52 and MAXIMUM age of 64 indicates **POSSIBLE INCONSISTENCY IN DATA** (who has a negative age, ins't it?). The ideal situation is to question Business Area and/or Data Area to clear it up. \n     - 3) Missing values detected in data (0.1% of missing values). However, this low percent won't cause any risk or impact on modeling\n     - 4) **Tendency**: according to scatter plot (above), the older the person is, the more likely he/she is to be considered as a good payer and vice-verse.\n     - 5) According to graphical visualization (scatter and boxplot), the feature 'AGE' does seem to be a candidate variable for predictor to explain the variability of y (Feature Selection), since there's differences between the variabilities of each label, 1 and 0. Therefore, such visualizations have the sensation of \"movement\" in both charts. The ideal situation is to TEST during the modeling.    \n<pre> </pre>\n    \n**LOAN**\n    \n- The feature varies between 1.38 and 13766.05, with a loan average ~ 4444.37. Important aspects when looking at data:\n     - 1) **quartiles** (Q1, Q2 and Q3) tell us of **quantity** and the respective value of **variation** around **1939.71 to 6432.41**;\n        - Interpretation Q1 tells us that 25% of data has a loan up to 1939.71 and 75% of data has an age above from it\n        - Interpretation Q2 tells us that 50% of data has a loan up to 3974.72 and 75% of data has an income above from it\n        - Interpretation Q3 tells us that 75% of data has a loan up to 6432.41 and 25% of data has an income above from it\n     - 2) The MINIMUM age of 1.38 and MAXIMUM age of 13766.05 does not indicate **POSSIBLE INCONSISTENCY IN DATA** once there is nothing within data that really catches my eye \n     - 3) No missing values (0% of missing values)\n     - 4) **Tendency**: according to scatter plot (above), any loan with values up to 2500 shows a trend towards good payers. On the other hand, values above 2500 do not make it possible to separate good and bad payers.\n     - 5) According to graphical visualization (scatter and boxplot), the feature 'AGE' does seem to be a candidate variable for predictor to explain the variability of y (Feature Selection), since there's differences between the variabilities of each label, 1 and 0. Therefore, such visualizations have the sensation of \"movement\" in both charts. The ideal situation is to TEST during the modeling.   \n","metadata":{"trusted":true}},{"cell_type":"code","source":"# Asymmetry x Kurtosis calculation \ndata_k_s = {'Asymmetry':  df[num_vars].skew(), 'Kurtosis': df[num_vars].kurtosis()}\ndf_k_s = pd.DataFrame (data_k_s, columns = ['Asymmetry','Kurtosis'])\ndf_k_s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values","metadata":{}},{"cell_type":"code","source":"(df.isnull().sum()* 100 / len(df)).sort_values(ascending=False).head(10).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Duplicate Analysis","metadata":{"trusted":true}},{"cell_type":"code","source":"df.drop(columns=\"clientid\",inplace=True)\n\nfeatures = df.columns.values[0:30]\nunique_max_train = []\nfor feature in features:\n    values = df[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n\nnp.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Value Counts Target","metadata":{"trusted":true}},{"cell_type":"code","source":"df['default'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = df.drop([\"default\"], axis=1)\ntrain_y = np.log1p(df[\"default\"].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\n\n## plotando as importÃ¢ncias ##\nfeat_names = df.columns.values\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_new_feature_distribution(df1, df2, label1, label2, features, n):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,n,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,n,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Baseline","metadata":{}},{"cell_type":"code","source":"def plot_roc(y_test,prob):\n    fpr, tpr, thresholds = roc_curve(y_test, probs)\n    # plot no skill\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    plt.plot(fpr, tpr, marker='.')\n    plt.title(\"ROC curve\")\n    plt.xlabel('false positive rate')\n    plt.ylabel('true positive rate')\n    # show the plot\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def logistic(X,y):\n    y_train=df['default'].astype('uint8')\n    X_train,X_test,y_train,y_test=train_test_split(df.drop('default',axis=1),y_train,test_size=.2,random_state=2020)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    prob=lr.predict_proba(X_test)\n    \n    roc=roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])\n    print('roc ',roc)\n\n    return (prob[:,1],y_test)\ny_train=df['default'].astype('uint8')\nprobs,y_test=logistic(df.drop('default',axis=1),y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(y_test,probs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Logistic Regression*","metadata":{}},{"cell_type":"code","source":"train_x, val_x, train_y, val_y=train_test_split(df.drop('default',axis=1),y_train,test_size=.2,random_state=2020)\n\nclf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\npred_y = clf.predict(val_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rocrl=roc_auc_score(val_y, pred_y)\nrocrl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, pred_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Random Forest Classifier*","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = RandomForestClassifier(n_estimators=220).fit(train_x,train_y)\npredictionforest = model.predict(val_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrocrf=roc_auc_score(val_y, predictionforest)\nprint('roc ',rocrf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, predictionforest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1, \n                            n_estimators = 220)\n\nxgb_cfl.fit(train_x, train_y)\ny_scorexgb = xgb_cfl.predict_proba(val_x)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rocxgb=roc_auc_score(val_y, y_scorexgb)\nprint('roc ',rocxgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, y_scorexgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grid Search CV","metadata":{}},{"cell_type":"code","source":"%%time\n# Logistic regression \nlog_cfl = LogisticRegression()\n\nparam_grid = {\n            'penalty' : ['l1','l2'], \n            'class_weight' : ['balanced', None], \n            'C' : [0.1, 1, 10, 100]\n            }\n\nCV_log_cfl = GridSearchCV(estimator = log_cfl, param_grid = param_grid , scoring = 'roc_auc', verbose = 1, n_jobs = -1)\nCV_log_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_log_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Logistic Regression*","metadata":{}},{"cell_type":"code","source":"log_cfl = LogisticRegression(C = best_parameters['C'], \n                             penalty = best_parameters['penalty'], \n                             class_weight = best_parameters['class_weight'])\n\nlog_cfl.fit(train_x, train_y)\ny_scoreLR = log_cfl.decision_function(val_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rocLR=roc_auc_score(val_y, y_scoreLR)\nprint('roc ',rocLR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, y_scoreLR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Random Forest Classifier*","metadata":{}},{"cell_type":"code","source":"rf_cfl = RandomForestClassifier(n_estimators=120).fit(train_x, train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_features': [2, 3],\n            'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 5, 10]\n            }\n\nCV_rnd_cfl = GridSearchCV(estimator = rf_cfl, param_grid = param_grid, scoring = 'roc_auc', verbose = 10, n_jobs = -1)\nCV_rnd_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_rnd_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_cfl = RandomForestClassifier(n_estimators = best_parameters['n_estimators'], \n                                 max_features = best_parameters['max_features'],  \n                                 min_samples_leaf = best_parameters['min_samples_leaf'],  \n                                 min_samples_split = 5)\nrf_cfl.fit(train_x, train_y)\ny_score = rf_cfl.predict_proba(val_x)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rocRF=roc_auc_score(val_y, y_score)\nprint('roc ',rocRF)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, y_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *XGBClassifier*","metadata":{}},{"cell_type":"code","source":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1)\nxgb_cfl.fit(train_x, train_y)\ny_pred = xgb_cfl.predict(val_x)\ny_score = xgb_cfl.predict_proba(val_x)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nparam_grid = {\n            'n_estimators': [50, 100, 200]\n              }\n\nCV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring ='roc_auc', verbose = 2)\nCV_xgb_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_xgb_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nparam_grid = {\n            'n_estimators': [50, 100, 200]\n              }\n\nCV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring ='roc_auc', verbose = 2)\nCV_xgb_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_xgb_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cfl2 = xgb.XGBClassifier(n_jobs = -1,n_estimators = 120)\n\nxgb_cfl2.fit(train_x, train_y)\ny_score2 = xgb_cfl.predict_proba(val_x)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rocxgb=roc_auc_score(val_y, y_score2)\nprint('roc ',rocxgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, y_score2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### *Voting Classifier*","metadata":{}},{"cell_type":"code","source":"voting_cfl = VotingClassifier (\n        estimators = [('xgb', xgb_cfl2), ('lt', log_cfl), ('rf', rf_cfl)],\n                     voting='soft', weights = [0.27, 0.23, 0.5])\n    \nvoting_cfl.fit(train_x, train_y)\n\ny_pred = voting_cfl.predict(val_x)\ny_score = voting_cfl.predict_proba(val_x)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rocV=roc_auc_score(val_y, y_score)\nprint('roc ',rocV)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(val_y, y_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}